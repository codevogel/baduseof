# Large Language Models don't know things

Recently I saw someone ask:

> "Hi, does my travel ticket for the Amsterdam region include Uitgeest? I'm
> planning a trip there next month. I asked the Google AI, and it said both yes
> and no."

This transforms what should be a one minute, four word search into a sprawling
collaborative effort involving dozens of people, multiple platforms, and several
data centers spinning up GPUs like they are summoning a small weather system.

Think about the data travel here:

This person typed their question into their phone. That question traveled across
networks, under oceans, through who knows how many kilometers of cable, until it
landed in a data center full of GPUs that woke up and started churning to
_predict_ whether Uitgeest is included in the Amsterdam Travel Region zone, just
so this person could avoid typing "Amsterdam Travel Region Ticket" into Google.
Turns out, it did not know. So they went to Reddit and did the entire proces
again, except now the question is sprayed across hundreds of devices, including
those that are in the hands of someone who is just trying to mind their own
business on the toilet.

Those strangers who are in the middle of relieving themselves then have to
perform that very same one minute Google search the original person avoided, and
send it back through another round of digital pinball so it can arrive back on
OP's phone.

Congratulations OP. You have just multiplied the effort and energy consumption
of a trivial task because you did not want to do it yourself. And it got you
nowhere.

And this is not limited to transit questions.

People ask Large Language Models basic math questions that a calculator could
solve faster and more reliably. (Mind you, calculators do not hallucinate
answers.) They literally ask this question from their _personal computing
device_, which is designed to do math, and lost
[human computers](<https://en.wikipedia.org/wiki/Computer_(occupation)>) their
jobs, to run some text model that tries to _predict_ what the result is of
`69/3`. Just let that settle in for a bit.

In a podcast, I heard one of the hosts talking about how they took a picture of
their bathroom while installing a washing machine, asked an AI to analyze the
image (which is already results in questionable results on its own), and then
provide installation instructions based on what it saw. The host then got
frustrated because the AI started talking about a hole that did not exist in the
bathroom wall. Meanwhile, the manual was literally taped to the machine.

Some more inane examples of people using AI for things they should not:

- Legal advice, as if a predictive text model understands regulations or
  consequences. Especially don't try to use
  [AI as a lawyer in court](https://www.youtube.com/shorts/MkmfZPt-gaw)

â€¢ Therapy. This is a difficult once, given there are global shortages of
therapists. Some people see it as a nice alternative while they're on the
waiting list. But this too can have
[drastic consequences](https://en.wikipedia.org/wiki/Chatbot_psychosis).

Let me be blunt: AI does not have feelings. It is not validating you. It is
producing statistically likely text that resembles validation. That is not the
same thing. And what's more, it's geared to please you. If you find emotional
comfort from reading nice words, that is fine. But do not confuse it with actual
support from a person who can understand you, and critique you where neccessary.

The core issue is not that people use AI. The issue is that people use AI while
they have no idea of its limitations, and start misjudging its capabilities
because it sounds oh-so confident.

> Q: So, when is it okay to use AI?

AI is excellent for:

- Summarizing texts you already know the contents of.
- Helping you write code, if you're capable of verifying the output.
- Menial tasks that require no precision, that have no existing solutions.

> Q: And when is it not okay to use AI?

Do not use AI when:

- Facts, correctness, and precision matters.
- There are already existing, deterministic solutions to your problem.
- You need advice that has real world consequences.

If your question has a right answer that affects your life, do not ask a
probabilistic word generator to hallucinate one.

In essence, it is akin to asking a Magic 8-Ball for life advice. The answers may
be in there, but they show up by chance, not by design. (Of course, take this
with a grain of salt, because a LLM is definitely more likely to produce a
correct answer than a Magic 8-Ball.)

Feel free to use AI. But study it's limitations. And do not let your own
capabilities get rusty because you outsource your thinking to a machine.

You are still allowed to think. Please do.
